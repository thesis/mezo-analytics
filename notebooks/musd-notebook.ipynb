{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from mezo.currency_utils import format_musd_currency_columns, get_token_price\n",
    "from mezo.datetime_utils import format_datetimes\n",
    "from mezo.data_utils import add_rolling_values, add_pct_change_columns, add_cumulative_columns\n",
    "from mezo.clients import SupabaseClient, BigQueryClient, SubgraphClient, Web3Client\n",
    "from scripts.get_raw_data import get_all_loans, get_liquidation_data, get_trove_liquidated_data\n",
    "load_dotenv(dotenv_path='../.env', override=True)\n",
    "COINGECKO_KEY = os.getenv('COINGECKO_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Trying troveUpdates query...\n",
      "Fetching transactions with skip=0...\n",
      "Fetching transactions with skip=1000...\n",
      "No more records found.\n",
      "✅ Found 366 loan records\n",
      "🔍 Trying liquidations query...\n",
      "Fetching transactions with skip=0...\n",
      "Fetching transactions with skip=1000...\n",
      "No more records found.\n",
      "✅ Found 2 liquidation records\n",
      "🔍 Trying troveLiquidateds query...\n",
      "Fetching transactions with skip=0...\n",
      "Fetching transactions with skip=1000...\n",
      "No more records found.\n",
      "✅ Found 2 trove liquidation records\n"
     ]
    }
   ],
   "source": [
    "# import raw data\n",
    "raw_loans = get_all_loans()\n",
    "raw_liquidations = get_liquidation_data()\n",
    "raw_troves_liquidated = get_trove_liquidated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def clean_loan_data(raw, sort_col, date_cols, currency_cols):\n",
    "    df = raw.copy().sort_values(by=sort_col, ascending=False)\n",
    "    df = format_datetimes(df, date_cols)\n",
    "    df = format_musd_currency_columns(df, currency_cols)\n",
    "    df['count'] = 1\n",
    "    df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_coll_ratio(df, token_id):\n",
    "    \"\"\"Computes the collateralization ratio\"\"\"\n",
    "    usd = get_token_price(token_id)\n",
    "    df['coll_usd'] = df['coll'] * usd\n",
    "    df['coll_ratio'] = (df['coll_usd']/df['principal'] ).fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_loans_subset(df, operation: int, equals):\n",
    "    \"\"\"Create a df with only new, adjusted, or closed loans\n",
    "    0 = opened, 1 = closed, 2 = adjusted\n",
    "    note: operation = 2 also includes liquidated loans, so we have to remove those manually\n",
    "    \"\"\"\n",
    "    df['operation'] = df['operation'].astype(int)\n",
    "    if equals is True:\n",
    "        adjusted = df.loc[df['operation'] == operation]\n",
    "    elif equals is False:\n",
    "        adjusted = df.loc[df['operation'] != operation]\n",
    "\n",
    "    return adjusted\n",
    "\n",
    "def process_liquidation_data(liquidations, troves_liquidated):\n",
    "    # Merge raw liquidation data from two queries\n",
    "    liquidation_df_merged = pd.merge(\n",
    "        liquidations, \n",
    "        troves_liquidated, \n",
    "        how='left', \n",
    "        on='transactionHash_'\n",
    "    )\n",
    "\n",
    "    liquidation_df_merged = liquidation_df_merged[\n",
    "        ['timestamp__x', \n",
    "        'liquidatedPrincipal', \n",
    "        'liquidatedInterest', \n",
    "        'liquidatedColl', \n",
    "        'borrower',\n",
    "        'transactionHash_',\n",
    "        'count_x'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    liquidations_df_final = liquidation_df_merged.rename(\n",
    "        columns = {\n",
    "            'timestamp__x': 'timestamp_', \n",
    "            'liquidatedPrincipal': 'principal', \n",
    "            'liquidatedInterest': 'interest',\n",
    "            'liquidatedColl': 'coll',\n",
    "            'count_x': 'count'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    liquidations_final = liquidations_df_final.copy()\n",
    "    liquidations_final['coll'] = liquidations_final['coll'].astype(float)\n",
    "\n",
    "    return liquidations_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/xx81t1c10qs96ft0v4m064800000gn/T/ipykernel_86233/894492014.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['operation'] = df['operation'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# clean raw data\n",
    "loans = clean_loan_data(\n",
    "    raw_loans, \n",
    "    sort_col='timestamp_', \n",
    "    date_cols=['timestamp_'], \n",
    "    currency_cols=['principal', 'coll', 'stake', 'interest']\n",
    ")\n",
    "\n",
    "loans = find_coll_ratio(loans, 'bitcoin')\n",
    "\n",
    "liquidations = clean_loan_data(\n",
    "    raw_liquidations,\n",
    "    sort_col='timestamp_',\n",
    "    date_cols=['timestamp_'],\n",
    "    currency_cols=['liquidatedPrincipal', 'liquidatedInterest', 'liquidatedColl']\n",
    ")\n",
    "\n",
    "troves_liquidated = clean_loan_data(\n",
    "    raw_troves_liquidated,\n",
    "    sort_col='timestamp_',\n",
    "    date_cols=['timestamp_'],\n",
    "    currency_cols=['debt', 'coll']\n",
    ")\n",
    "\n",
    "# Create df for liquidated loans\n",
    "liquidations_final = process_liquidation_data(liquidations, troves_liquidated)\n",
    "\n",
    "# Create df's for new loans, closed loans, and adjusted loans and upload to BigQuery\n",
    "new_loans = get_loans_subset(loans, 0, True)\n",
    "closed_loans = get_loans_subset(loans, 1, True)\n",
    "adjusted_loans = get_loans_subset(loans, 2, True) # Only adjusted loans (incl multiple adjustments from a single user)\n",
    "\n",
    "## Remove liquidations from adjusted loans\n",
    "liquidated_borrowers = liquidations_final['borrower'].unique()\n",
    "adjusted_loans = adjusted_loans[~adjusted_loans['borrower'].isin(liquidated_borrowers)]\n",
    "\n",
    "##################################\n",
    "\n",
    "# Get latest loans\n",
    "latest_loans = loans.drop_duplicates(subset='borrower', keep='first')\n",
    "\n",
    "# Create df with only open loans\n",
    "latest_open_loans = get_loans_subset(latest_loans, 1, False)\n",
    "\n",
    "# Remove liquidated loans from list of latest loans w/o closed loans\n",
    "latest_open_loans = latest_open_loans[~latest_open_loans['borrower'].isin(liquidated_borrowers)]\n",
    "\n",
    "##################################\n",
    "\n",
    "# Break down adjusted loan types for analysis\n",
    "adjusted_loans = adjusted_loans.sort_values(by=['borrower', 'timestamp_'])\n",
    "first_tx = adjusted_loans.groupby('borrower').first().reset_index()\n",
    "\n",
    "adjusted_loans_merged = adjusted_loans.merge(\n",
    "    first_tx[['borrower', 'principal', 'coll']], \n",
    "    on='borrower', \n",
    "    suffixes=('', '_initial')\n",
    ")\n",
    "\n",
    "## Loan increases\n",
    "increased_loans = adjusted_loans_merged[adjusted_loans_merged['principal'] \n",
    "                                        > adjusted_loans_merged['principal_initial']].copy()\n",
    "increased_loans['type'] = 1\n",
    "\n",
    "## Collateral changes\n",
    "coll_increased = adjusted_loans_merged[adjusted_loans_merged['coll'] \n",
    "                                       > adjusted_loans_merged['coll_initial']].copy()\n",
    "coll_increased['type'] = 2\n",
    "\n",
    "coll_decreased = adjusted_loans_merged[adjusted_loans_merged['coll'] \n",
    "                                       < adjusted_loans_merged['coll_initial']].copy()\n",
    "coll_decreased['type'] = 3\n",
    "\n",
    "## MUSD Repayments\n",
    "principal_decreased = adjusted_loans_merged[adjusted_loans_merged['principal'] \n",
    "                                            < adjusted_loans_merged['principal_initial']].copy()\n",
    "principal_decreased['type'] = 4\n",
    "\n",
    "## Create final_adjusted_loans dataframe with type column\n",
    "final_adjusted_loans = pd.concat([\n",
    "    increased_loans,\n",
    "    coll_increased, \n",
    "    coll_decreased,\n",
    "    principal_decreased\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily dataframe\n",
    "daily_new_loans = new_loans.groupby(['timestamp_']).agg(\n",
    "    loans_opened = ('count', 'sum'),\n",
    "    borrowers = ('borrower', lambda x: x.nunique()),\n",
    "    principal = ('principal', 'sum'),\n",
    "    collateral = ('coll', 'sum'),\n",
    "    interest = ('interest', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "daily_closed_loans = closed_loans.groupby(['timestamp_']).agg(\n",
    "    loans_closed = ('count', 'sum'),\n",
    "    borrowers_who_closed = ('borrower', lambda x: x.nunique())\n",
    ").reset_index()\n",
    "\n",
    "daily_new_and_closed_loans = pd.merge(daily_new_loans, daily_closed_loans, how = 'outer', on = 'timestamp_').fillna(0)\n",
    "daily_new_and_closed_loans[['loans_opened', 'borrowers', 'loans_closed', 'borrowers_who_closed']] = daily_new_and_closed_loans[['loans_opened', 'borrowers', 'loans_closed', 'borrowers_who_closed']].astype('int')      \n",
    "daily_adjusted_loans = adjusted_loans.groupby(['timestamp_']).agg(\n",
    "    loans_adjusted = ('count', 'sum'),\n",
    "    borrowers_who_adjusted = ('borrower', lambda x: x.nunique())\n",
    ").reset_index()\n",
    "\n",
    "daily_loan_data = pd.merge(daily_new_and_closed_loans, daily_adjusted_loans, how='outer', on='timestamp_').fillna(0)\n",
    "daily_loan_data[['loans_adjusted', 'borrowers_who_adjusted']] = daily_loan_data[['loans_adjusted', 'borrowers_who_adjusted']].astype(int)\n",
    "\n",
    "daily_balances = latest_loans.groupby(['timestamp_']).agg(\n",
    "    musd = ('principal', 'sum'),\n",
    "    interest = ('interest', 'sum'),\n",
    "    collateral = ('coll', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "daily_balances = daily_balances.rename(\n",
    "    columns={'musd': 'net_musd', \n",
    "             'interest': 'net_interest',\n",
    "             'collateral': 'net_coll'}\n",
    ")\n",
    "\n",
    "daily_loans_merged = pd.merge(daily_loan_data, daily_balances, how='outer', on='timestamp_')\n",
    "\n",
    "cols = {\n",
    "    'timestamp_': 'date', \n",
    "    'principal': 'gross_musd', \n",
    "    'collateral': 'gross_coll', \n",
    "    'interest': 'gross_interest',\n",
    "    'borrowers_who_closed': 'closers', \n",
    "    'borrowers_who_adjusted': 'adjusters'\n",
    "}\n",
    "\n",
    "daily_loans_merged = daily_loans_merged.rename(columns = cols)\n",
    "\n",
    "daily_musd_final = add_rolling_values(daily_loans_merged, 30, ['net_musd', 'net_interest', 'net_coll']).fillna(0)\n",
    "daily_musd_final_2 = add_cumulative_columns(daily_musd_final, ['net_musd', 'net_interest', 'net_coll'])\n",
    "daily_musd_final_3 = add_pct_change_columns(daily_musd_final_2, ['net_musd', 'net_interest', 'net_coll'], 'daily').fillna(0)\n",
    "final_daily_musd = daily_musd_final_3.replace([float('inf'), -float('inf')], 0)\n",
    "final_daily_musd['date'] = pd.to_datetime(final_daily_musd['date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_BORROW_FEES = \"\"\"\n",
    "query getBorrowFees ($skip: Int!) {\n",
    "  borrowingFeePaids (\n",
    "    orderBy: timestamp_\n",
    "    orderDirection: desc\n",
    "    first: 1000\n",
    "    skip: $skip\n",
    "  ){\n",
    "    timestamp_\n",
    "    fee\n",
    "    borrower\n",
    "    transactionHash_\n",
    "  }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "musd = SubgraphClient(\n",
    "    url=SubgraphClient.BORROWER_OPS_SUBGRAPH, \n",
    "    headers= SubgraphClient.SUBGRAPH_HEADERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching transactions with skip=0...\n",
      "Fetching transactions with skip=1000...\n",
      "No more records found.\n"
     ]
    }
   ],
   "source": [
    "fees =  musd.fetch_subgraph_data(GET_BORROW_FEES, 'borrowingFeePaids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 293 fee records\n"
     ]
    }
   ],
   "source": [
    "if fees:\n",
    "    fees = pd.DataFrame(fees)\n",
    "    print(f\"✅ Found {len(fees)} fee records\")\n",
    "else:\n",
    "    print(\"⚠️ Query returned no data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_fees = fees.copy()\n",
    "loan_fees.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpd\u001b[49m.merge(loans, loan_fees, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m, on=\u001b[33m'\u001b[39m\u001b[33mtransactionHash_\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.merge(loans, loan_fees, how='left', on='transactionHash_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
